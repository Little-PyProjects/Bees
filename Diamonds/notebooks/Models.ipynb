{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3cb0f21f",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3d7d9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This isn't strictly needed. However it solves this annoying pandas error:\n",
    "\n",
    "/opt/homebrew/Caskroom/miniforge/base/envs/supervised/lib/python3.9/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
    "  from pandas import MultiIndex, Int64Index\n",
    "  \n",
    "The problem is solved with xgboost 1.6 but I don't want to use pip in this case and the conda package is currently 1.5.1  \n",
    "'''\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99cfed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import xgboost as xgb\n",
    "\n",
    "\n",
    "from sklearn import preprocessing \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9095eda7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/diamonds.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a710426e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c8001c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state=314)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a9b4f1c",
   "metadata": {},
   "source": [
    "### Linear Regression for a baseline\n",
    "```\n",
    "72.4 ms ± 5.24 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
    "\n",
    "RMSE: 16057.10618458078\n",
    "R2  : 0.7728174575988863\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc7d65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg_model = LinearRegression()\n",
    "model = lin_reg_model.fit(X_train, y_train)\n",
    "y_hat= model.predict(X_test)\n",
    "\n",
    "print(\"RMSE: {}\".format(np.sqrt(mean_squared_error((y_test),(y_hat)))))\n",
    "print(\"R2  : {}\".format(np.sqrt(r2_score((y_test),(y_hat)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7fc8c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# leave this commented unless you have a few extra minutes to spare\n",
    "\n",
    "# rr  = RandomForestRegressor()\n",
    "# rr.fit(X_train,y_train)\n",
    "# y_pred = rr.predict(X_test)\n",
    "\n",
    "# print(\"RMSE: {}\".format(np.sqrt(mean_squared_error((y_test),(y_pred)))))\n",
    "# print(\"R2  : {}\".format(np.sqrt(r2_score((y_test),(y_pred)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4a0e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\"\"\n",
    "Random Forest Regression has a {round((16057.1061-9536.7866)/16057.1061 *100 , 1)} % improvement over baseline in RMSE \n",
    "and a {round((.9262440179416279-.7728174575988863)/.7728174575988863 *100 , 1)} % improvement in R2\n",
    "      \n",
    "... but takes about 1,000x longer to run.\"\"\")\n",
    "\n",
    "# Check these numbers because it's 3AM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1b51cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Decision Trees - marginally better than LR. Fast but max out at depth of 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb2dd4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "\n",
    "tree=DecisionTreeRegressor(max_depth=3)\n",
    "tree.fit(X_train, y_train)\n",
    "\n",
    "y_pred = tree.predict(X_test)\n",
    "\n",
    "print(\"RMSE: {}\".format(np.sqrt(mean_squared_error((y_test),(y_pred)))))\n",
    "print(\"R2  : {}\".format(np.sqrt(r2_score((y_test),(y_pred)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1d64802",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Decision Trees has a 13.3 % improvement over baseline in RMSE \n",
      "and a 10.9 % improvement in R2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"\"\"\n",
    "Decision Trees has a {round((16057.1061-13921.2573)/16057.1061 *100 , 1)} % improvement over baseline in RMSE \n",
    "and a {round((.9262440179416279-.8350244922620658)/.8350244922620658 *100 , 1)} % improvement in R2\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2b61be",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_classifier = xgb.XGBClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d632b04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_r = xgb.XGBRegressor(objective ='reg:squarederror', n_estimators = 100, seed = 123,)\n",
    "xgb_r.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4402b3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = xgb_r.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c223debc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"RMSE: {}\".format(np.sqrt(mean_squared_error((y_test),(y_pred)))))\n",
    "print(\"R2  : {}\".format(np.sqrt(r2_score((y_test),(y_pred)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7598f38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### XGBoost regression is strong for two reasons. Good performance and so much faster than Random Forrest Regression. And this is for a basically untuned model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b32bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# params = {  'max_depth': [2,3,4],\n",
    "#             'learning_rate': [0.1, 0.2, 0.3, 0.4],\n",
    "#             'n_estimators': [100],\n",
    "#             'colsample_bytree': [0.3, 0.7],\n",
    "#             'subsample':}\n",
    "\n",
    "# clf = GridSearchCV(estimator=xgbr, \n",
    "#                    param_grid=params,\n",
    "#                    scoring='neg_mean_squared_error', \n",
    "#                    verbose=1)\n",
    "# clf.fit(X, y)\n",
    "\n",
    "# print(\"Best parameters:\", clf.best_params_)\n",
    "# print(\"Lowest RMSE: \", (-clf.best_score_)**(1/2.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c922a4",
   "metadata": {},
   "source": [
    "Yeah, I don't get that.\n",
    "```\n",
    "Fitting 5 folds for each of 54 candidates, totalling 270 fits\n",
    "Best parameters: {'colsample_bytree': 0.7, 'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 1000}\n",
    "Lowest RMSE:  25154.080799724477\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4335cc91",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Rational Quadratic Kernel seems a good contender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80af7ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.gaussian_process.kernels import RationalQuadratic\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86bf2108",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition of the Rational Quadratic Kernel\n",
    "kernel = 1.0 * RationalQuadratic (length_scale=1.0, alpha=0.1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "696cffc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Gaussian Process\n",
    "gpc = GaussianProcessClassifier(kernel=kernel).fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3edbe32",
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = RationalQuadratic(length_scale=1.0, alpha=1.5)\n",
    "gpc = GaussianProcessClassifier(kernel=kernel,\n",
    "        random_state=0).fit(X, y)\n",
    "gpc.score(X, y)\n",
    "\n",
    "gpc.predict_proba(X[:2,:])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
